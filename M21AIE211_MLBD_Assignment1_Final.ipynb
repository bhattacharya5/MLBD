{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPA+8P5XGA0JMqIIMJaEVLV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharya5/MLBD/blob/main/M21AIE211_MLBD_Assignment1_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BhkQZhriQLJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/gdrive/My Drive/Colab Notebooks/semester3/MLBD\""
      ],
      "metadata": {
        "id": "gNdXwrMbiS_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# python magic function\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "iTzDQouZiTsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/semester3/MLBD/train.csv')\n",
        "df_train.head(5)"
      ],
      "metadata": {
        "id": "lDTsSVjMiVJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "w-BrqHxBiXRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.iloc[0:5000]\n",
        "df_train.shape"
      ],
      "metadata": {
        "id": "xWiNNf5Tib5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/semester3/MLBD/test.csv')\n",
        "df_test.head(5)"
      ],
      "metadata": {
        "id": "T5iFQ-0qieZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_test.iloc[0:3000]\n",
        "df_test.shape"
      ],
      "metadata": {
        "id": "b1aBPGP7iiSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = df_train[\"label\"]\n",
        "\n",
        "# Drop 'label' column\n",
        "X_train = df_train.drop(labels = [\"label\"],axis = 1) \n",
        "\n",
        "X_test = df_test"
      ],
      "metadata": {
        "id": "7exQBVETil3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#plot numbers"
      ],
      "metadata": {
        "id": "OPvydJOMi0_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = to_categorical(Y_train, num_classes = 10)\n",
        "Y_train[8]"
      ],
      "metadata": {
        "id": "Uc-D6SPtiqvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create figure with 3x3 subplots using matplotlib.pyplot\n",
        "fig, axs = plt.subplots(3, 3, figsize = (12, 12))\n",
        "plt.gray()\n",
        "\n",
        "# loop through subplots and add mnist images\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.matshow(X_train[i])\n",
        "    ax.axis('off')\n",
        "    ax.set_title('Number {}'.format(Y_train[i]))\n",
        "    \n",
        "# display the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "CgzQKsYni4ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BFR implementation\n",
        "\n",
        "MNIST is a dataset of handwritten digits, and each image is of size 28x28 pixels. The dataset has 60,000 training images and 10,000 test images.\n",
        "\n",
        "Preprocess the data by scaling the pixel values to a range between 0 and 1, and flattening the images to a one-dimensional array of size 784. You can also reduce the dimensionality of the data using techniques like PCA or t-SNE if needed.\n",
        "\n",
        "For implementing the BFR clustering algorithm, here is a brief overview of the algorithm:\n",
        "\n",
        "Initialize the algorithm with a set of random centroids.\n",
        "\n",
        "Assign each data point to the nearest centroid.\n",
        "\n",
        "Compute the quality of the clustering using a cost function like the sum of squared errors.\n",
        "\n",
        "If the quality of the clustering is below a threshold, split the centroids into sub-clusters using a splitting algorithm like k-means or hierarchical clustering.\n",
        "\n",
        "If the quality of the clustering is above a threshold, merge the centroids into larger clusters using a merging algorithm like agglomerative clustering.\n",
        "\n",
        "Repeat the above steps until convergence or a maximum number of iterations is reached.\n",
        "\n",
        "Train the BFR algorithm on the MNIST training dataset.\n",
        "\n",
        "Predict the labels of the test dataset by assigning each test image to the nearest centroid and using the label of the centroid as the predicted label for the test image."
      ],
      "metadata": {
        "id": "EMm_AS4Oi5hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "X= X_train / 255.0  # Scale pixel values to [0, 1]\n",
        "y= Y_train.astype('int')\n",
        "\n",
        "# Reduce dimensionality using PCA\n",
        "pca = PCA(n_components=50)\n",
        "X = pca.fit_transform(X)\n",
        "\n",
        "# Define BFR clustering algorithm\n",
        "class BFR:\n",
        "    def __init__(self, k_init, split_size, merge_size, max_iter):\n",
        "        self.k_init = k_init\n",
        "        self.split_size = split_size\n",
        "        self.merge_size = merge_size\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Initialize centroids using KMeans\n",
        "        kmeans = KMeans(n_clusters=self.k_init, n_init=1)\n",
        "        kmeans.fit(X)\n",
        "        centroids = kmeans.cluster_centers_\n",
        "        labels = kmeans.labels_\n",
        "        n_clusters = self.k_init\n",
        "\n",
        "        # Run BFR algorithm\n",
        "        for i in range(self.max_iter):\n",
        "            # Assign points to nearest centroid\n",
        "            distances = np.linalg.norm(X[:, np.newaxis, :] - centroids, axis=2)\n",
        "            assignments = np.argmin(distances, axis=1)\n",
        "\n",
        "            # Split large clusters\n",
        "            for j in range(n_clusters):\n",
        "                if np.sum(assignments == j) > self.split_size:\n",
        "                    sub_kmeans = KMeans(n_clusters=2, n_init=1)\n",
        "                    sub_kmeans.fit(X[assignments == j])\n",
        "                    sub_centroids = sub_kmeans.cluster_centers_\n",
        "                    sub_labels = sub_kmeans.labels_\n",
        "                    sub_n_clusters = 2\n",
        "\n",
        "                    # Update centroids and labels\n",
        "                    centroids[j] = sub_centroids[0]\n",
        "                    centroids = np.vstack((centroids, sub_centroids[1]))\n",
        "                    labels[assignments == j] = sub_labels + n_clusters\n",
        "                    n_clusters += 1\n",
        "\n",
        "            # Merge small clusters\n",
        "            for j in range(n_clusters):\n",
        "                if np.sum(labels == j) < self.merge_size:\n",
        "                    closest_cluster = np.argmin(np.linalg.norm(centroids[j] - centroids, axis=1))\n",
        "                    labels[labels == j] = closest_cluster\n",
        "\n",
        "            # Recompute centroids\n",
        "            for j in range(n_clusters):\n",
        "                centroids[j] = np.mean(X[labels == j], axis=0)\n",
        "\n",
        "        # Save centroids and labels\n",
        "        self.centroids = centroids\n",
        "        self.labels = labels\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Assign points to nearest centroid and return labels\n",
        "        distances = np.linalg.norm(X[:, np.newaxis, :] - self.centroids, axis=2)\n",
        "        assignments = np.argmin(distances, axis=1)\n",
        "        return self.labels[assignments]\n",
        "\n",
        "# Train BFR model on MNIST dataset\n",
        "bfr = BFR(k_init=50, split_size=50, merge_size=10, max_iter=10)\n",
        "bfr.fit(X)"
      ],
      "metadata": {
        "id": "_c8q-twPi8Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict labels of test dataset\n",
        "y_pred = bfr.predict(pca.transform(df_test))"
      ],
      "metadata": {
        "id": "AqAfvxTEjT05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CURE Implementation\n",
        "\n",
        "\n",
        "In this implementation, we first load the MNIST dataset assuming a limit of K1 samples in main memory at a time.\n",
        "\n",
        "Then, we define the CURE function that takes the input data X, the number of clusters num_clusters, and K1 samples in main memory. Inside the CURE function, we first randomly select one point as the first medoid, and then use the farthest point sampling to select s-1 more medoids. We then perform AgglomerativeClustering clustering on the reduced set of medoids to obtain the final clustering labels.\n",
        "\n",
        "Finally, we call the CURE function with K1 samples=10 and number of clusters=10 to cluster the MNIST dataset into 10 clusters. We print the size of each cluster to display the clustering results."
      ],
      "metadata": {
        "id": "H5HcMmASrS-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "def CURE_clustering(data, K1, num_clusters):\n",
        "    # Step 1: Randomly select K1 points from the dataset to be the initial representatives\n",
        "    representatives = data[np.random.choice(data.shape[0], K1, replace=False)]\n",
        "    num_points = data.shape[0]\n",
        "    \n",
        "    # Initialize cluster assignments for all data points\n",
        "    cluster_assignments = np.zeros(num_points, dtype=int)\n",
        "    \n",
        "    # Step 2: Assign each point in the dataset to the closest representative point\n",
        "    for i in range(num_points):\n",
        "        distances = np.linalg.norm(data[i] - representatives, axis=1)\n",
        "        cluster_assignments[i] = np.argmin(distances)\n",
        "        \n",
        "    # Step 3: For each cluster, select a number of points that are farthest away from the representative and mark them as \"outliers\"\n",
        "    outliers = []\n",
        "    for cluster in range(K1):\n",
        "        cluster_indices = np.where(cluster_assignments == cluster)[0]\n",
        "        if len(cluster_indices) > 1:\n",
        "            distances = np.linalg.norm(data[cluster_indices] - representatives[cluster], axis=1)\n",
        "            sorted_indices = np.argsort(distances)[::-1]\n",
        "            outliers.extend(cluster_indices[sorted_indices][:int(len(cluster_indices)/2)])\n",
        "    \n",
        "    # Step 4: For each cluster, use these outliers to compute a new representative point\n",
        "    new_representatives = np.zeros((num_clusters, data.shape[1]))\n",
        "    for cluster in range(K1):\n",
        "        cluster_indices = np.where(cluster_assignments == cluster)[0]\n",
        "        if len(cluster_indices) > 1:\n",
        "            outlier_indices = [index for index in cluster_indices if index in outliers]\n",
        "            non_outlier_indices = [index for index in cluster_indices if index not in outliers]\n",
        "            if len(outlier_indices) > 0:\n",
        "                outliers_data = data[outlier_indices]\n",
        "                representative = np.mean(outliers_data, axis=0)\n",
        "                new_representatives[cluster] = representative\n",
        "            else:\n",
        "                representative = representatives[cluster]\n",
        "                new_representatives[cluster] = representative\n",
        "    \n",
        "    # Perform agglomerative clustering on the new representatives\n",
        "    clustering = AgglomerativeClustering(n_clusters=num_clusters).fit(new_representatives)\n",
        "    \n",
        "    # Assign each data point to a cluster based on the closest representative point\n",
        "    final_assignments = np.zeros(num_points, dtype=int)\n",
        "    for i in range(num_points):\n",
        "        distances = np.linalg.norm(data[i] - new_representatives, axis=1)\n",
        "        cluster = clustering.labels_[np.argmin(distances)]\n",
        "        final_assignments[i] = cluster\n",
        "    \n",
        "    return final_assignments"
      ],
      "metadata": {
        "id": "H2UhBmm8rW-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= X_train / 255.0  # Scale pixel values to [0, 1]\n",
        "y= Y_train.astype('int')"
      ],
      "metadata": {
        "id": "gFb05-e3rZ4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "o4I2xPafrctW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform CURE clustering on the data\n",
        "num_clusters = 10\n",
        "final_assignments = CURE_clustering(X_scaled, 10, num_clusters)"
      ],
      "metadata": {
        "id": "LJqbMtZprfsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After clustering, calculate the percentage of samples from each class and convert it into probability values."
      ],
      "metadata": {
        "id": "pM8bW0G1FYjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming you have the true labels of the samples in y_true\n",
        "unique_labels = np.unique(final_assignments) # Get the unique labels\n",
        "class_counts = np.zeros(len(unique_labels)) # Initialize an array to hold the class counts\n",
        "for i, label in enumerate(unique_labels):\n",
        "    class_counts[i] = np.count_nonzero(final_assignments == label)\n"
      ],
      "metadata": {
        "id": "vNZRcmarFX0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_percentages = class_counts / len(final_assignments) * 100\n",
        "class_probabilities = class_percentages / 100\n",
        "\n",
        "print(class_percentages, class_probabilities)"
      ],
      "metadata": {
        "id": "bI57JQ6OFeA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using these, calculate the entropy of each cluster. Also calculate the total entropy of all the clusters by summing the entropy of individual clusters."
      ],
      "metadata": {
        "id": "RywRz00jFmwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cluster_entropy(labels):\n",
        "    n_labels = len(labels)\n",
        "\n",
        "    if n_labels <= 1:\n",
        "        return 0\n",
        "\n",
        "    counts = np.bincount(labels)\n",
        "    probs = counts / n_labels\n",
        "    n_classes = np.count_nonzero(probs)\n",
        "\n",
        "    if n_classes <= 1:\n",
        "        return 0\n",
        "\n",
        "    ent = 0.\n",
        "\n",
        "    # Compute entropy\n",
        "    for i in probs:\n",
        "        if i > 0.:\n",
        "            ent -= i * np.log2(i)\n",
        "\n",
        "    return ent\n",
        "\n",
        "\n",
        "unique_labels = set(final_assignments)\n",
        "for label in unique_labels:\n",
        "    mask = (final_assignments == label)    \n",
        "    cluster = y[mask]\n",
        "    entropy = cluster_entropy(cluster)\n",
        "    print(\"Cluster \", label, \" entropy: \", entropy)\n"
      ],
      "metadata": {
        "id": "EegDqBdVFzbI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}